{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow Lite Sentiment Analysis Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Refer to https://www.tensorflow.org/lite/tutorials/model_maker_text_classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from tflite_model_maker import model_spec\n",
    "from tflite_model_maker import text_classifier\n",
    "from tflite_model_maker.config import ExportFormat\n",
    "from tflite_model_maker.text_classifier import AverageWordVecSpec\n",
    "from tflite_model_maker.text_classifier import DataLoader\n",
    "\n",
    "import tensorflow as tf\n",
    "assert tf.__version__.startswith('2')\n",
    "tf.get_logger().setLevel('ERROR')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download dataset. We are training on SST-2 (see https://deepai.org/dataset/stanford-sentiment-treebank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = tf.keras.utils.get_file(\n",
    "      fname='SST-2.zip',\n",
    "      origin='https://dl.fbaipublicfiles.com/glue/data/SST-2.zip',\n",
    "      extract=True)\n",
    "data_dir = os.path.join(os.path.dirname(data_dir), 'SST-2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the dataset into a Pandas dataframe and change the current label names (0 and 1) to a more human-readable ones (negative and positive) and use them for model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def replace_label(original_file, new_file):\n",
    "    # Load the original file to pandas. We need to specify the separator as\n",
    "    # '\\t' as the training data is stored in TSV format\n",
    "    df = pd.read_csv(original_file, sep='\\t')\n",
    "\n",
    "    # Define how we want to change the label name\n",
    "    label_map = {0: 'negative', 1: 'positive'}\n",
    "\n",
    "    # Excute the label change\n",
    "    df.replace({'label': label_map}, inplace=True)\n",
    "\n",
    "    # Write the updated dataset to a new file\n",
    "    df.to_csv(new_file)\n",
    "\n",
    "# Replace the label name for both the training and test dataset. Then write the\n",
    "# updated CSV dataset to the current folder.\n",
    "replace_label(os.path.join(os.path.join(data_dir, 'train.tsv')), 'train.csv')\n",
    "replace_label(os.path.join(os.path.join(data_dir, 'dev.tsv')), 'dev.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 1. Choose a text classification model architecture.**\n",
    "* Here we use the average word embedding model architecture, which will produce a small and fast model with decent accuracy. Other options include BERT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "spec = model_spec.get('average_word_vec')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 2. Load the training and test data, then preprocess them according to a specific model_spec.**\n",
    "* We will load the training and test dataset with the human-readable label name that were created earlier.\n",
    "* DataLoader reads the requirement from model_spec and automatically executes the necessary preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = DataLoader.from_csv(\n",
    "      filename='train.csv',\n",
    "      text_column='sentence',\n",
    "      label_column='label',\n",
    "      model_spec=spec,\n",
    "      is_training=True)\n",
    "\n",
    "test_data = DataLoader.from_csv(\n",
    "      filename='dev.csv',\n",
    "      text_column='sentence',\n",
    "      label_column='label',\n",
    "      model_spec=spec,\n",
    "      is_training=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 3. Train the TensorFlow model with the training data.**\n",
    "* The average word embedding model use batch_size = 32 by default.\n",
    "* It takes 2104 steps to go through the 67,349 sentences in the training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "2104/2104 [==============================] - ETA: 16:28 - loss: 0.6923 - accuracy: 0.468 - ETA: 3:08 - loss: 0.6908 - accuracy: 0.578 - ETA: 10s - loss: 0.6917 - accuracy: 0.5167 - ETA: 5s - loss: 0.6906 - accuracy: 0.531 - ETA: 3s - loss: 0.6894 - accuracy: 0.54 - ETA: 3s - loss: 0.6892 - accuracy: 0.54 - ETA: 3s - loss: 0.6878 - accuracy: 0.54 - ETA: 2s - loss: 0.6884 - accuracy: 0.54 - ETA: 2s - loss: 0.6883 - accuracy: 0.54 - ETA: 2s - loss: 0.6878 - accuracy: 0.55 - ETA: 2s - loss: 0.6878 - accuracy: 0.55 - ETA: 2s - loss: 0.6879 - accuracy: 0.55 - ETA: 1s - loss: 0.6880 - accuracy: 0.55 - ETA: 1s - loss: 0.6878 - accuracy: 0.55 - ETA: 1s - loss: 0.6874 - accuracy: 0.55 - ETA: 1s - loss: 0.6871 - accuracy: 0.55 - ETA: 1s - loss: 0.6870 - accuracy: 0.55 - ETA: 1s - loss: 0.6871 - accuracy: 0.55 - ETA: 1s - loss: 0.6865 - accuracy: 0.55 - ETA: 1s - loss: 0.6864 - accuracy: 0.55 - ETA: 1s - loss: 0.6863 - accuracy: 0.55 - ETA: 1s - loss: 0.6861 - accuracy: 0.55 - ETA: 1s - loss: 0.6859 - accuracy: 0.55 - ETA: 1s - loss: 0.6855 - accuracy: 0.55 - ETA: 0s - loss: 0.6852 - accuracy: 0.55 - ETA: 0s - loss: 0.6850 - accuracy: 0.55 - ETA: 0s - loss: 0.6848 - accuracy: 0.55 - ETA: 0s - loss: 0.6845 - accuracy: 0.55 - ETA: 0s - loss: 0.6842 - accuracy: 0.55 - ETA: 0s - loss: 0.6838 - accuracy: 0.55 - ETA: 0s - loss: 0.6834 - accuracy: 0.55 - ETA: 0s - loss: 0.6830 - accuracy: 0.55 - ETA: 0s - loss: 0.6826 - accuracy: 0.56 - ETA: 0s - loss: 0.6822 - accuracy: 0.56 - ETA: 0s - loss: 0.6818 - accuracy: 0.56 - ETA: 0s - loss: 0.6813 - accuracy: 0.56 - ETA: 0s - loss: 0.6808 - accuracy: 0.56 - ETA: 0s - loss: 0.6802 - accuracy: 0.56 - ETA: 0s - loss: 0.6796 - accuracy: 0.56 - ETA: 0s - loss: 0.6787 - accuracy: 0.56 - ETA: 0s - loss: 0.6779 - accuracy: 0.56 - ETA: 0s - loss: 0.6770 - accuracy: 0.57 - 3s 1ms/step - loss: 0.6763 - accuracy: 0.5725\n",
      "Epoch 2/10\n",
      "2104/2104 [==============================] - ETA: 4s - loss: 0.6514 - accuracy: 0.68 - ETA: 2s - loss: 0.6407 - accuracy: 0.64 - ETA: 1s - loss: 0.6387 - accuracy: 0.65 - ETA: 1s - loss: 0.6366 - accuracy: 0.65 - ETA: 1s - loss: 0.6331 - accuracy: 0.65 - ETA: 1s - loss: 0.6306 - accuracy: 0.65 - ETA: 1s - loss: 0.6282 - accuracy: 0.66 - ETA: 1s - loss: 0.6264 - accuracy: 0.66 - ETA: 1s - loss: 0.6241 - accuracy: 0.66 - ETA: 1s - loss: 0.6213 - accuracy: 0.66 - ETA: 1s - loss: 0.6200 - accuracy: 0.67 - ETA: 1s - loss: 0.6173 - accuracy: 0.67 - ETA: 1s - loss: 0.6155 - accuracy: 0.67 - ETA: 1s - loss: 0.6132 - accuracy: 0.67 - ETA: 1s - loss: 0.6112 - accuracy: 0.67 - ETA: 1s - loss: 0.6089 - accuracy: 0.68 - ETA: 1s - loss: 0.6062 - accuracy: 0.68 - ETA: 1s - loss: 0.6045 - accuracy: 0.68 - ETA: 1s - loss: 0.6027 - accuracy: 0.68 - ETA: 1s - loss: 0.6006 - accuracy: 0.68 - ETA: 0s - loss: 0.5984 - accuracy: 0.69 - ETA: 0s - loss: 0.5961 - accuracy: 0.69 - ETA: 0s - loss: 0.5938 - accuracy: 0.69 - ETA: 0s - loss: 0.5916 - accuracy: 0.69 - ETA: 0s - loss: 0.5891 - accuracy: 0.69 - ETA: 0s - loss: 0.5868 - accuracy: 0.69 - ETA: 0s - loss: 0.5845 - accuracy: 0.70 - ETA: 0s - loss: 0.5825 - accuracy: 0.70 - ETA: 0s - loss: 0.5803 - accuracy: 0.70 - ETA: 0s - loss: 0.5782 - accuracy: 0.70 - ETA: 0s - loss: 0.5760 - accuracy: 0.70 - ETA: 0s - loss: 0.5740 - accuracy: 0.70 - ETA: 0s - loss: 0.5722 - accuracy: 0.71 - ETA: 0s - loss: 0.5702 - accuracy: 0.71 - ETA: 0s - loss: 0.5679 - accuracy: 0.71 - ETA: 0s - loss: 0.5660 - accuracy: 0.71 - ETA: 0s - loss: 0.5641 - accuracy: 0.71 - ETA: 0s - loss: 0.5624 - accuracy: 0.71 - ETA: 0s - loss: 0.5603 - accuracy: 0.71 - ETA: 0s - loss: 0.5585 - accuracy: 0.72 - 2s 956us/step - loss: 0.5573 - accuracy: 0.7211\n",
      "Epoch 3/10\n",
      "2104/2104 [==============================] - ETA: 4s - loss: 0.5028 - accuracy: 0.71 - ETA: 2s - loss: 0.4829 - accuracy: 0.78 - ETA: 2s - loss: 0.4800 - accuracy: 0.77 - ETA: 2s - loss: 0.4771 - accuracy: 0.78 - ETA: 1s - loss: 0.4760 - accuracy: 0.78 - ETA: 1s - loss: 0.4741 - accuracy: 0.78 - ETA: 1s - loss: 0.4747 - accuracy: 0.78 - ETA: 1s - loss: 0.4714 - accuracy: 0.78 - ETA: 1s - loss: 0.4716 - accuracy: 0.78 - ETA: 1s - loss: 0.4704 - accuracy: 0.78 - ETA: 1s - loss: 0.4679 - accuracy: 0.78 - ETA: 1s - loss: 0.4686 - accuracy: 0.78 - ETA: 1s - loss: 0.4673 - accuracy: 0.78 - ETA: 1s - loss: 0.4660 - accuracy: 0.78 - ETA: 1s - loss: 0.4657 - accuracy: 0.78 - ETA: 1s - loss: 0.4644 - accuracy: 0.78 - ETA: 1s - loss: 0.4635 - accuracy: 0.78 - ETA: 1s - loss: 0.4630 - accuracy: 0.78 - ETA: 1s - loss: 0.4620 - accuracy: 0.78 - ETA: 1s - loss: 0.4602 - accuracy: 0.78 - ETA: 0s - loss: 0.4592 - accuracy: 0.78 - ETA: 0s - loss: 0.4586 - accuracy: 0.78 - ETA: 0s - loss: 0.4580 - accuracy: 0.78 - ETA: 0s - loss: 0.4564 - accuracy: 0.78 - ETA: 0s - loss: 0.4552 - accuracy: 0.79 - ETA: 0s - loss: 0.4542 - accuracy: 0.79 - ETA: 0s - loss: 0.4532 - accuracy: 0.79 - ETA: 0s - loss: 0.4524 - accuracy: 0.79 - ETA: 0s - loss: 0.4514 - accuracy: 0.79 - ETA: 0s - loss: 0.4507 - accuracy: 0.79 - ETA: 0s - loss: 0.4496 - accuracy: 0.79 - ETA: 0s - loss: 0.4492 - accuracy: 0.79 - ETA: 0s - loss: 0.4487 - accuracy: 0.79 - ETA: 0s - loss: 0.4479 - accuracy: 0.79 - ETA: 0s - loss: 0.4474 - accuracy: 0.79 - ETA: 0s - loss: 0.4460 - accuracy: 0.79 - ETA: 0s - loss: 0.4453 - accuracy: 0.79 - ETA: 0s - loss: 0.4444 - accuracy: 0.79 - ETA: 0s - loss: 0.4433 - accuracy: 0.79 - ETA: 0s - loss: 0.4422 - accuracy: 0.79 - 2s 957us/step - loss: 0.4418 - accuracy: 0.7983\n",
      "Epoch 4/10\n",
      "2104/2104 [==============================] - ETA: 6s - loss: 0.3262 - accuracy: 0.90 - ETA: 1s - loss: 0.3981 - accuracy: 0.82 - ETA: 1s - loss: 0.4018 - accuracy: 0.82 - ETA: 1s - loss: 0.4045 - accuracy: 0.82 - ETA: 1s - loss: 0.4078 - accuracy: 0.81 - ETA: 1s - loss: 0.4037 - accuracy: 0.81 - ETA: 1s - loss: 0.4044 - accuracy: 0.81 - ETA: 1s - loss: 0.4024 - accuracy: 0.82 - ETA: 1s - loss: 0.4024 - accuracy: 0.82 - ETA: 1s - loss: 0.4012 - accuracy: 0.82 - ETA: 1s - loss: 0.4019 - accuracy: 0.82 - ETA: 1s - loss: 0.4042 - accuracy: 0.81 - ETA: 1s - loss: 0.4041 - accuracy: 0.81 - ETA: 1s - loss: 0.4035 - accuracy: 0.81 - ETA: 1s - loss: 0.4040 - accuracy: 0.81 - ETA: 1s - loss: 0.4030 - accuracy: 0.81 - ETA: 1s - loss: 0.4036 - accuracy: 0.81 - ETA: 1s - loss: 0.4042 - accuracy: 0.81 - ETA: 1s - loss: 0.4035 - accuracy: 0.82 - ETA: 1s - loss: 0.4026 - accuracy: 0.82 - ETA: 1s - loss: 0.4025 - accuracy: 0.82 - ETA: 0s - loss: 0.4026 - accuracy: 0.82 - ETA: 0s - loss: 0.4031 - accuracy: 0.82 - ETA: 0s - loss: 0.4017 - accuracy: 0.82 - ETA: 0s - loss: 0.4015 - accuracy: 0.82 - ETA: 0s - loss: 0.4009 - accuracy: 0.82 - ETA: 0s - loss: 0.3997 - accuracy: 0.82 - ETA: 0s - loss: 0.3991 - accuracy: 0.82 - ETA: 0s - loss: 0.3992 - accuracy: 0.82 - ETA: 0s - loss: 0.3985 - accuracy: 0.82 - ETA: 0s - loss: 0.3975 - accuracy: 0.82 - ETA: 0s - loss: 0.3972 - accuracy: 0.82 - ETA: 0s - loss: 0.3970 - accuracy: 0.82 - ETA: 0s - loss: 0.3967 - accuracy: 0.82 - ETA: 0s - loss: 0.3963 - accuracy: 0.82 - ETA: 0s - loss: 0.3968 - accuracy: 0.82 - ETA: 0s - loss: 0.3958 - accuracy: 0.82 - ETA: 0s - loss: 0.3956 - accuracy: 0.82 - ETA: 0s - loss: 0.3953 - accuracy: 0.82 - ETA: 0s - loss: 0.3948 - accuracy: 0.82 - ETA: 0s - loss: 0.3945 - accuracy: 0.82 - 2s 978us/step - loss: 0.3943 - accuracy: 0.8266\n",
      "Epoch 5/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2104/2104 [==============================] - ETA: 4s - loss: 0.3227 - accuracy: 0.84 - ETA: 1s - loss: 0.3843 - accuracy: 0.83 - ETA: 1s - loss: 0.3783 - accuracy: 0.83 - ETA: 1s - loss: 0.3774 - accuracy: 0.83 - ETA: 1s - loss: 0.3782 - accuracy: 0.83 - ETA: 1s - loss: 0.3749 - accuracy: 0.83 - ETA: 1s - loss: 0.3757 - accuracy: 0.83 - ETA: 1s - loss: 0.3762 - accuracy: 0.83 - ETA: 1s - loss: 0.3742 - accuracy: 0.83 - ETA: 1s - loss: 0.3731 - accuracy: 0.83 - ETA: 1s - loss: 0.3746 - accuracy: 0.83 - ETA: 1s - loss: 0.3765 - accuracy: 0.83 - ETA: 1s - loss: 0.3759 - accuracy: 0.83 - ETA: 1s - loss: 0.3765 - accuracy: 0.83 - ETA: 1s - loss: 0.3748 - accuracy: 0.83 - ETA: 1s - loss: 0.3765 - accuracy: 0.83 - ETA: 1s - loss: 0.3773 - accuracy: 0.83 - ETA: 1s - loss: 0.3766 - accuracy: 0.83 - ETA: 1s - loss: 0.3756 - accuracy: 0.83 - ETA: 0s - loss: 0.3750 - accuracy: 0.83 - ETA: 0s - loss: 0.3752 - accuracy: 0.83 - ETA: 0s - loss: 0.3762 - accuracy: 0.83 - ETA: 0s - loss: 0.3745 - accuracy: 0.83 - ETA: 0s - loss: 0.3748 - accuracy: 0.83 - ETA: 0s - loss: 0.3745 - accuracy: 0.83 - ETA: 0s - loss: 0.3744 - accuracy: 0.83 - ETA: 0s - loss: 0.3740 - accuracy: 0.83 - ETA: 0s - loss: 0.3735 - accuracy: 0.83 - ETA: 0s - loss: 0.3732 - accuracy: 0.83 - ETA: 0s - loss: 0.3722 - accuracy: 0.83 - ETA: 0s - loss: 0.3726 - accuracy: 0.83 - ETA: 0s - loss: 0.3725 - accuracy: 0.83 - ETA: 0s - loss: 0.3725 - accuracy: 0.83 - ETA: 0s - loss: 0.3727 - accuracy: 0.83 - ETA: 0s - loss: 0.3723 - accuracy: 0.83 - ETA: 0s - loss: 0.3718 - accuracy: 0.84 - ETA: 0s - loss: 0.3715 - accuracy: 0.84 - ETA: 0s - loss: 0.3714 - accuracy: 0.84 - ETA: 0s - loss: 0.3709 - accuracy: 0.84 - ETA: 0s - loss: 0.3707 - accuracy: 0.84 - 2s 946us/step - loss: 0.3706 - accuracy: 0.8414\n",
      "Epoch 6/10\n",
      "2104/2104 [==============================] - ETA: 4s - loss: 0.3205 - accuracy: 0.84 - ETA: 1s - loss: 0.3633 - accuracy: 0.85 - ETA: 1s - loss: 0.3540 - accuracy: 0.85 - ETA: 1s - loss: 0.3580 - accuracy: 0.84 - ETA: 1s - loss: 0.3599 - accuracy: 0.84 - ETA: 1s - loss: 0.3567 - accuracy: 0.84 - ETA: 1s - loss: 0.3575 - accuracy: 0.84 - ETA: 1s - loss: 0.3564 - accuracy: 0.84 - ETA: 1s - loss: 0.3548 - accuracy: 0.84 - ETA: 1s - loss: 0.3553 - accuracy: 0.84 - ETA: 1s - loss: 0.3575 - accuracy: 0.84 - ETA: 1s - loss: 0.3596 - accuracy: 0.84 - ETA: 1s - loss: 0.3591 - accuracy: 0.84 - ETA: 1s - loss: 0.3586 - accuracy: 0.84 - ETA: 1s - loss: 0.3576 - accuracy: 0.84 - ETA: 1s - loss: 0.3596 - accuracy: 0.84 - ETA: 1s - loss: 0.3603 - accuracy: 0.84 - ETA: 1s - loss: 0.3589 - accuracy: 0.84 - ETA: 1s - loss: 0.3582 - accuracy: 0.84 - ETA: 0s - loss: 0.3580 - accuracy: 0.84 - ETA: 0s - loss: 0.3581 - accuracy: 0.84 - ETA: 0s - loss: 0.3592 - accuracy: 0.84 - ETA: 0s - loss: 0.3580 - accuracy: 0.84 - ETA: 0s - loss: 0.3578 - accuracy: 0.84 - ETA: 0s - loss: 0.3577 - accuracy: 0.84 - ETA: 0s - loss: 0.3574 - accuracy: 0.84 - ETA: 0s - loss: 0.3574 - accuracy: 0.84 - ETA: 0s - loss: 0.3570 - accuracy: 0.84 - ETA: 0s - loss: 0.3564 - accuracy: 0.84 - ETA: 0s - loss: 0.3569 - accuracy: 0.84 - ETA: 0s - loss: 0.3569 - accuracy: 0.84 - ETA: 0s - loss: 0.3571 - accuracy: 0.84 - ETA: 0s - loss: 0.3574 - accuracy: 0.84 - ETA: 0s - loss: 0.3568 - accuracy: 0.84 - ETA: 0s - loss: 0.3567 - accuracy: 0.84 - ETA: 0s - loss: 0.3562 - accuracy: 0.84 - ETA: 0s - loss: 0.3561 - accuracy: 0.84 - ETA: 0s - loss: 0.3559 - accuracy: 0.85 - 2s 913us/step - loss: 0.3555 - accuracy: 0.8501\n",
      "Epoch 7/10\n",
      "2104/2104 [==============================] - ETA: 6s - loss: 0.2292 - accuracy: 0.90 - ETA: 1s - loss: 0.3421 - accuracy: 0.85 - ETA: 1s - loss: 0.3453 - accuracy: 0.85 - ETA: 1s - loss: 0.3481 - accuracy: 0.85 - ETA: 1s - loss: 0.3499 - accuracy: 0.85 - ETA: 1s - loss: 0.3464 - accuracy: 0.85 - ETA: 1s - loss: 0.3454 - accuracy: 0.85 - ETA: 1s - loss: 0.3434 - accuracy: 0.85 - ETA: 1s - loss: 0.3457 - accuracy: 0.85 - ETA: 1s - loss: 0.3444 - accuracy: 0.85 - ETA: 1s - loss: 0.3438 - accuracy: 0.85 - ETA: 1s - loss: 0.3472 - accuracy: 0.85 - ETA: 1s - loss: 0.3462 - accuracy: 0.85 - ETA: 1s - loss: 0.3468 - accuracy: 0.85 - ETA: 1s - loss: 0.3469 - accuracy: 0.85 - ETA: 1s - loss: 0.3469 - accuracy: 0.85 - ETA: 1s - loss: 0.3478 - accuracy: 0.85 - ETA: 1s - loss: 0.3473 - accuracy: 0.85 - ETA: 1s - loss: 0.3461 - accuracy: 0.85 - ETA: 1s - loss: 0.3460 - accuracy: 0.85 - ETA: 0s - loss: 0.3452 - accuracy: 0.85 - ETA: 0s - loss: 0.3459 - accuracy: 0.85 - ETA: 0s - loss: 0.3466 - accuracy: 0.85 - ETA: 0s - loss: 0.3457 - accuracy: 0.85 - ETA: 0s - loss: 0.3458 - accuracy: 0.85 - ETA: 0s - loss: 0.3459 - accuracy: 0.85 - ETA: 0s - loss: 0.3456 - accuracy: 0.85 - ETA: 0s - loss: 0.3457 - accuracy: 0.85 - ETA: 0s - loss: 0.3454 - accuracy: 0.85 - ETA: 0s - loss: 0.3445 - accuracy: 0.85 - ETA: 0s - loss: 0.3447 - accuracy: 0.85 - ETA: 0s - loss: 0.3448 - accuracy: 0.85 - ETA: 0s - loss: 0.3448 - accuracy: 0.85 - ETA: 0s - loss: 0.3454 - accuracy: 0.85 - ETA: 0s - loss: 0.3448 - accuracy: 0.85 - ETA: 0s - loss: 0.3449 - accuracy: 0.85 - ETA: 0s - loss: 0.3446 - accuracy: 0.85 - ETA: 0s - loss: 0.3444 - accuracy: 0.85 - ETA: 0s - loss: 0.3445 - accuracy: 0.85 - 2s 939us/step - loss: 0.3441 - accuracy: 0.8573\n",
      "Epoch 8/10\n",
      "2104/2104 [==============================] - ETA: 4s - loss: 0.3432 - accuracy: 0.90 - ETA: 1s - loss: 0.3440 - accuracy: 0.86 - ETA: 1s - loss: 0.3398 - accuracy: 0.86 - ETA: 1s - loss: 0.3386 - accuracy: 0.86 - ETA: 1s - loss: 0.3420 - accuracy: 0.85 - ETA: 1s - loss: 0.3383 - accuracy: 0.85 - ETA: 1s - loss: 0.3389 - accuracy: 0.85 - ETA: 1s - loss: 0.3380 - accuracy: 0.85 - ETA: 1s - loss: 0.3364 - accuracy: 0.86 - ETA: 1s - loss: 0.3358 - accuracy: 0.86 - ETA: 1s - loss: 0.3384 - accuracy: 0.86 - ETA: 1s - loss: 0.3406 - accuracy: 0.85 - ETA: 1s - loss: 0.3403 - accuracy: 0.85 - ETA: 1s - loss: 0.3401 - accuracy: 0.86 - ETA: 1s - loss: 0.3397 - accuracy: 0.86 - ETA: 1s - loss: 0.3413 - accuracy: 0.85 - ETA: 1s - loss: 0.3420 - accuracy: 0.85 - ETA: 1s - loss: 0.3406 - accuracy: 0.86 - ETA: 1s - loss: 0.3397 - accuracy: 0.86 - ETA: 0s - loss: 0.3395 - accuracy: 0.86 - ETA: 0s - loss: 0.3395 - accuracy: 0.86 - ETA: 0s - loss: 0.3405 - accuracy: 0.86 - ETA: 0s - loss: 0.3397 - accuracy: 0.86 - ETA: 0s - loss: 0.3399 - accuracy: 0.86 - ETA: 0s - loss: 0.3399 - accuracy: 0.86 - ETA: 0s - loss: 0.3394 - accuracy: 0.86 - ETA: 0s - loss: 0.3393 - accuracy: 0.86 - ETA: 0s - loss: 0.3392 - accuracy: 0.86 - ETA: 0s - loss: 0.3383 - accuracy: 0.86 - ETA: 0s - loss: 0.3386 - accuracy: 0.86 - ETA: 0s - loss: 0.3390 - accuracy: 0.86 - ETA: 0s - loss: 0.3389 - accuracy: 0.86 - ETA: 0s - loss: 0.3399 - accuracy: 0.86 - ETA: 0s - loss: 0.3390 - accuracy: 0.86 - ETA: 0s - loss: 0.3392 - accuracy: 0.86 - ETA: 0s - loss: 0.3390 - accuracy: 0.86 - ETA: 0s - loss: 0.3388 - accuracy: 0.86 - ETA: 0s - loss: 0.3389 - accuracy: 0.86 - 2s 914us/step - loss: 0.3383 - accuracy: 0.8617\n",
      "Epoch 9/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2104/2104 [==============================] - ETA: 6s - loss: 0.2757 - accuracy: 0.87 - ETA: 2s - loss: 0.3292 - accuracy: 0.86 - ETA: 2s - loss: 0.3336 - accuracy: 0.85 - ETA: 2s - loss: 0.3314 - accuracy: 0.86 - ETA: 1s - loss: 0.3337 - accuracy: 0.86 - ETA: 1s - loss: 0.3349 - accuracy: 0.85 - ETA: 1s - loss: 0.3325 - accuracy: 0.86 - ETA: 1s - loss: 0.3299 - accuracy: 0.86 - ETA: 1s - loss: 0.3307 - accuracy: 0.86 - ETA: 1s - loss: 0.3305 - accuracy: 0.86 - ETA: 1s - loss: 0.3303 - accuracy: 0.86 - ETA: 1s - loss: 0.3323 - accuracy: 0.86 - ETA: 1s - loss: 0.3319 - accuracy: 0.86 - ETA: 1s - loss: 0.3328 - accuracy: 0.86 - ETA: 1s - loss: 0.3327 - accuracy: 0.86 - ETA: 1s - loss: 0.3329 - accuracy: 0.86 - ETA: 1s - loss: 0.3335 - accuracy: 0.86 - ETA: 1s - loss: 0.3334 - accuracy: 0.86 - ETA: 1s - loss: 0.3318 - accuracy: 0.86 - ETA: 1s - loss: 0.3317 - accuracy: 0.86 - ETA: 0s - loss: 0.3319 - accuracy: 0.86 - ETA: 0s - loss: 0.3330 - accuracy: 0.86 - ETA: 0s - loss: 0.3328 - accuracy: 0.86 - ETA: 0s - loss: 0.3328 - accuracy: 0.86 - ETA: 0s - loss: 0.3323 - accuracy: 0.86 - ETA: 0s - loss: 0.3323 - accuracy: 0.86 - ETA: 0s - loss: 0.3326 - accuracy: 0.86 - ETA: 0s - loss: 0.3322 - accuracy: 0.86 - ETA: 0s - loss: 0.3318 - accuracy: 0.86 - ETA: 0s - loss: 0.3315 - accuracy: 0.86 - ETA: 0s - loss: 0.3316 - accuracy: 0.86 - ETA: 0s - loss: 0.3317 - accuracy: 0.86 - ETA: 0s - loss: 0.3318 - accuracy: 0.86 - ETA: 0s - loss: 0.3324 - accuracy: 0.86 - ETA: 0s - loss: 0.3318 - accuracy: 0.86 - ETA: 0s - loss: 0.3319 - accuracy: 0.86 - ETA: 0s - loss: 0.3321 - accuracy: 0.86 - ETA: 0s - loss: 0.3317 - accuracy: 0.86 - ETA: 0s - loss: 0.3314 - accuracy: 0.86 - 2s 928us/step - loss: 0.3314 - accuracy: 0.8637\n",
      "Epoch 10/10\n",
      "2104/2104 [==============================] - ETA: 6s - loss: 0.3529 - accuracy: 0.90 - ETA: 2s - loss: 0.3225 - accuracy: 0.86 - ETA: 2s - loss: 0.3222 - accuracy: 0.86 - ETA: 2s - loss: 0.3286 - accuracy: 0.86 - ETA: 1s - loss: 0.3279 - accuracy: 0.86 - ETA: 1s - loss: 0.3284 - accuracy: 0.86 - ETA: 1s - loss: 0.3270 - accuracy: 0.86 - ETA: 1s - loss: 0.3257 - accuracy: 0.86 - ETA: 1s - loss: 0.3259 - accuracy: 0.86 - ETA: 1s - loss: 0.3243 - accuracy: 0.86 - ETA: 1s - loss: 0.3240 - accuracy: 0.86 - ETA: 1s - loss: 0.3267 - accuracy: 0.86 - ETA: 1s - loss: 0.3279 - accuracy: 0.86 - ETA: 1s - loss: 0.3264 - accuracy: 0.86 - ETA: 1s - loss: 0.3280 - accuracy: 0.86 - ETA: 1s - loss: 0.3278 - accuracy: 0.86 - ETA: 1s - loss: 0.3286 - accuracy: 0.86 - ETA: 1s - loss: 0.3297 - accuracy: 0.86 - ETA: 1s - loss: 0.3290 - accuracy: 0.86 - ETA: 1s - loss: 0.3275 - accuracy: 0.86 - ETA: 1s - loss: 0.3276 - accuracy: 0.86 - ETA: 0s - loss: 0.3272 - accuracy: 0.86 - ETA: 0s - loss: 0.3285 - accuracy: 0.86 - ETA: 0s - loss: 0.3282 - accuracy: 0.86 - ETA: 0s - loss: 0.3282 - accuracy: 0.86 - ETA: 0s - loss: 0.3283 - accuracy: 0.86 - ETA: 0s - loss: 0.3283 - accuracy: 0.86 - ETA: 0s - loss: 0.3278 - accuracy: 0.86 - ETA: 0s - loss: 0.3276 - accuracy: 0.86 - ETA: 0s - loss: 0.3274 - accuracy: 0.86 - ETA: 0s - loss: 0.3270 - accuracy: 0.86 - ETA: 0s - loss: 0.3263 - accuracy: 0.86 - ETA: 0s - loss: 0.3268 - accuracy: 0.86 - ETA: 0s - loss: 0.3266 - accuracy: 0.86 - ETA: 0s - loss: 0.3266 - accuracy: 0.86 - ETA: 0s - loss: 0.3269 - accuracy: 0.86 - ETA: 0s - loss: 0.3260 - accuracy: 0.86 - ETA: 0s - loss: 0.3263 - accuracy: 0.86 - ETA: 0s - loss: 0.3264 - accuracy: 0.86 - ETA: 0s - loss: 0.3263 - accuracy: 0.86 - ETA: 0s - loss: 0.3260 - accuracy: 0.86 - 2s 984us/step - loss: 0.3256 - accuracy: 0.8658\n"
     ]
    }
   ],
   "source": [
    "model = text_classifier.create(train_data, model_spec=spec, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 4. Evaluate the model with the test data.**\n",
    "* Default batch size is 32, it will take 28 steps to go through the 872 sentences in the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28/28 [==============================] 0.2001 - accuracy: 0.90 - 0s 776us/step - loss: 0.5163 - accuracy: 0.8337\n"
     ]
    }
   ],
   "source": [
    "loss, acc = model.evaluate(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss = 0.5162637233734131 \n",
      "Accuracy= 0.8337156176567078\n"
     ]
    }
   ],
   "source": [
    "print('Loss = {} \\nAccuracy= {}'.format(loss, acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 5. Export as a TensorFlow Lite model.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.export(export_dir='average_word_vec')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* This model can be integrated into an Android app using the NLClassifier API of the TensorFlow Lite Task Library.\n",
    "\n",
    "* See the TFLite Text Classification sample app for more details on how the model is used in a working app.\n",
    "\n",
    "* Note 1: Android Studio Model Binding does not support text classification yet so please use the TensorFlow Lite Task Library.\n",
    "\n",
    "* Note 2: There is a model.json file in the same folder with the TFLite model. It contains the JSON representation of the metadata bundled inside the TensorFlow Lite model. Model metadata helps the TFLite Task Library know what the model does and how to pre-process/post-process data for the model. You don't need to download the model.json file as it is only for informational purpose and its content is already inside the TFLite file.\n",
    "\n",
    "* Note 3: If you train a text classification model using MobileBERT or BERT-Base architecture, you will need to use BertNLClassifier API instead to integrate the trained model into a mobile app."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
